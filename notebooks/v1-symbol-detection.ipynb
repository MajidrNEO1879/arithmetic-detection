{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb8095",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "from sympy import *\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    " \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf41ac87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/MajidrNEO1879/arithmetic-detection.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f31089",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CocoDetectionDataset(Dataset):\n",
    "    # Init function: loads annotation file and prepares list of image IDs\n",
    "    def __init__(self, image_dir, annotation_path, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.coco = COCO(annotation_path)\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "        self.transforms = transforms\n",
    " \n",
    "    # Returns total number of images\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    " \n",
    "    # Fetches a single image and its annotations\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.image_dir, image_info['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    " \n",
    "        # Load all annotations for this image\n",
    "        annotation_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        annotations = self.coco.loadAnns(annotation_ids)\n",
    " \n",
    "        # Extract bounding boxes and labels from annotations\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in annotations:\n",
    "            xmin, ymin, width, height = obj['bbox']\n",
    "            xmax = xmin + width\n",
    "            ymax = ymin + height\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(obj['category_id'])\n",
    " \n",
    "        # Convert annotations to PyTorch tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        area = torch.as_tensor([obj['area'] for obj in annotations],    dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor([obj.get('iscrowd', 0) for obj in annotations], dtype=torch.int64)\n",
    " \n",
    "        # Package everything into a target dictionary\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    " \n",
    "        # Apply transforms if any were passed\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    " \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0079f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return ToTensor()\n",
    " \n",
    "# Load training dataset\n",
    "train_dataset = CocoDetectionDataset(\n",
    "    image_dir=\"/content/arithmetic-detection/train\", \n",
    "    annotation_path=\"/content/arithmetic-detection/train/_annotations.coco.json\",\n",
    "    transforms=get_transform()\n",
    ")\n",
    " \n",
    "# Load validation dataset\n",
    "val_dataset = CocoDetectionDataset(\n",
    "    image_dir=\"/content/arithmetic-detection/valid\",\n",
    "    annotation_path=\"/content/arithmetic-detection/valid/_annotations.coco.json\",\n",
    "    transforms=get_transform()\n",
    ")\n",
    "# Load dataset with DataLoaders, you can change batch_size \n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f25f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "category_ids = train_dataset.coco.getCatIds()\n",
    "num_classes = len(category_ids)\n",
    "categories = train_dataset.coco.loadCats(category_ids)\n",
    "class_names = [cat['name'] for cat in categories]\n",
    "\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Class names:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222e446",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_loader))\n",
    " \n",
    "# loop through one batch and draw bounding boxes and labels\n",
    "for i in range(len(images)):\n",
    "    # CxHxW --> HxWxC\n",
    "    image = images[i].permute(1, 2, 0).numpy()   \n",
    "    # Rescale\n",
    "    image = (image * 255).astype(np.uint8)   \n",
    "    # Convert RGB to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "     \n",
    "    # get bounding box coordinates and labels\n",
    "    boxes = targets[i]['boxes']\n",
    "    labels = targets[i]['labels']\n",
    " \n",
    "    for box, label in zip(boxes, labels):\n",
    "        x1, y1, x2, y2 = map(int, box.tolist())\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"Class {label.item()}\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    " \n",
    "    # Show image with bboxes using matplotlib\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Sample {i + 1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650cd47",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    ")\n",
    "# Number of classes in the dataset (including background)\n",
    "# +1 for bg class\n",
    "num_classes = len(train_dataset.coco.getCatIds()) + 1 \n",
    " \n",
    "# Number of input features for the classifier head\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    " \n",
    "\"\"\"  \n",
    "Number of classes must be equal to your label number\n",
    "\"\"\"\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    " \n",
    "# Move the model to the GPU for faster training\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce690b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get parameters that require gradients (the model's trainable parameters)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    " \n",
    "# Define the optimizer SGD(Stochastic Gradient Descent) \n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e43767",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/pytorch/vision.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a4178",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/vision/references/detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766e55d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32935ad8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# num_epochs = 10\n",
    "\n",
    "# # Loop through each epoch\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "#     # Train the model for one epoch, printing status every 25 iterations\n",
    "#     train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=50)  # Using train_loader for training\n",
    "\n",
    "#     # Evaluate the model only on the validation dataset, not training\n",
    "#     evaluate(model, val_loader, device=device)  # Using val_loader for evaluation\n",
    "\n",
    "#     # save the model after each epoch\n",
    "#     torch.save(model.state_dict(), f\"model_symbols_epochs_{epoch + 1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff5502",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "\n",
    "# class names\n",
    "label_list= ['Symb', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-', '--', 'a', 'arcctg', 'b', 'c', 'cos', \n",
    "             'ctg', 'd', 'div', 'drob', 'e', 'eight', 'eqv', 'f', 'five', 'four', 'g', 'i', 'integ', 'j', 'ln', 'log', 'm', 'minus', 'mult', \n",
    "             'nine', 'one', 'other_class', 'pi', 'plus', 'seven', 'sin', 'six', 't', 'tg', 'three', 'two', 'u', 'x', 'x-', 'y', 'z', 'zero']\n",
    "\n",
    "# Number of classes (include background)\n",
    "num_classes = 56\n",
    "\n",
    "# Load the same model\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=num_classes)\n",
    "\n",
    "# Load trained Faster R-CNN model\n",
    "model.load_state_dict(torch.load(r\"/content/model_symbols_epochs_9.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Load image with OpenCV and convert to RGB\n",
    "img_path = r\"/content/test_inputs/2025_11_10_11j_Kleki.png\" # CHANGE this to your image path\n",
    "image_bgr = cv2.imread(img_path)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "image_pil = Image.fromarray(image_rgb)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "image_tensor = transform(image_pil).unsqueeze(0)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(image_tensor)\n",
    "\n",
    "# detection data\n",
    "boxes = predictions[0]['boxes']\n",
    "labels = predictions[0]['labels']\n",
    "scores = predictions[0]['scores']\n",
    "\n",
    "print(\"=== PREDICTION DEBUG INFO ===\")\n",
    "print(f\"Number of detections: {len(predictions[0]['boxes'])}\")\n",
    "print(f\"All scores: {predictions[0]['scores']}\")\n",
    "print(f\"All labels: {predictions[0]['labels']}\")\n",
    "\n",
    "\"\"\"\n",
    "Higher threshold give you more accurate detections,\n",
    "but number of predictions is reduced; there is a simple trade-off\n",
    "\"\"\"\n",
    "#thresholds = [0.8, 0.5, 0.3, 0.1]\n",
    "# detections_found = False\n",
    "# for threshold in thresholds:\n",
    "#     print(f\"\\nTrying threshold: {threshold}\")\n",
    "#     detections = 0\n",
    "\n",
    "#     for i in range(len(boxes)):\n",
    "#         if scores[i] > threshold:\n",
    "#             detections += 1\n",
    "#             box = boxes[i].cpu().numpy().astype(int)\n",
    "#             label = label_list[labels[i]]\n",
    "#             score = scores[i].item()\n",
    "#             print(f\"Detection {detections}: {label} with score {score:.4f}\")\n",
    "\n",
    "#     if detections > 0:\n",
    "#         print(f\"Found {detections} detection(s) with threshold {threshold}\")\n",
    "#         detections_found = True\n",
    "#         break\n",
    "\n",
    "# if not detections_found:\n",
    "#     print(\"No detections found even with threshold 0.1!\")\n",
    "#     print(\"Possible issues:\")\n",
    "#     print(\"1. Model didn't learn properly\")\n",
    "#     print(\"2. Image doesn't contain the object\")\n",
    "#     print(\"3. Training data issues\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
